---
title: "Prepare scUTRquant - RPE1 Day 7 Essential"
author: "Mervin M Fansler"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document: 
    code_folding: show
    toc: true
    toc_float: true
---

# Purpose

We process the various sources of metadata to produce files compatible with running scUTRquant.

Please note the final URLs are automatically checked. The `S3_IDS` parameter controls
which S3 locations are used (e.g., `sra-pub-src-1` or `sra-pub-src-2`). Currently, 
1 and 2 appear sufficient; in other cases, this may need to be adjusted.

# Initialization

## Libraries
```{r libs, message=FALSE, warning=FALSE}
library(magrittr)
library(tidyverse)
library(purrr)
library(rhdf5)
library(httr)
```

## Parameters
```{r set_params}
set.seed(20210818)
## AWS S3 search locations
URL_STR="https://sra-pub-src-%d.s3.amazonaws.com/%s/%s_possorted_genome_bam.bam.1"
S3_IDS=c(1,2)

## input files
AD_FILE="data/hdf5/rpe1_raw_singlecell_01.h5ad"
CSV_SRA_TABLE="metadata/rd7-essential-run-table.csv"
CSV_RAW_FILES="metadata/RD7_raw_files.csv"

## output files
CSV_RUNS_SHEET="metadata/rd7-essential-runs-sheet.csv"
CSV_SAMPLE_SHEET="metadata/rd7-essential-sample-sheet.csv"
CSV_CELL_ANNOTS="metadata/rd7-essential-cell-annots.csv.gz"
```

## Functions

```{r methods}
read_ad_df <- function (file, name) {
    x_attrs <- h5readAttributes(file, name)
    
    ## check requested entry is a dataframe
    ## TODO: do we need to check encoding-version?
    stopifnot(x_attrs[['encoding-type']] == "dataframe")
    
    ## rownames and columns in order
    idx_cols <- unlist(x_attrs[c("_index", "column-order")], use.names=FALSE)
    
    ## load the factor levels
    x_levels <- h5read(file, str_c(name, "/__categories"))
    
    ## load dataframe
    h5read(file, name)[idx_cols] %>% as_tibble() %>%
        ## replace categorical columns with proper factors
        mutate(across(any_of(names(x_levels)), ~ factor(x_levels[[cur_column()]][.x+1L])))
}

is_valid_url <- function (x) {
    res <- HEAD(x)
    res$status_code %in% c(200, 400)
}

get_valid_url <- function (srr, run_id, s3=c(1,2)) {
    url <- character()
    urls <- map(s3, ~ sprintf(URL_STR, .x, srr, run_id))
    for (u in urls) {
        if (is_valid_url(u)) {
            url <- u
            break
        }
    }
    url
}
```

# Data
## Loading
```{r load_data, message=FALSE}
df_cells <- read_ad_df(AD_FILE, '/obs')
df_runs <- read_csv(CSV_SRA_TABLE, col_select=c("Run", "Library Name"))
df_samples <- read_csv(CSV_RAW_FILES, col_select=1:2)
```

## Preprocessing
```{r prepare_data}
df_samples %<>% dplyr::rename(run_id=library, gem_group=gemgroup) %>%
    mutate(gem_group=as.integer(gem_group))
df_runs %<>% dplyr::rename(srr=Run, run_id=`Library Name`)

df_runs_clean <- df_runs %>% 
    left_join(df_samples, by="run_id") %>%
    mutate(sample_id=sprintf("RD7_%02d", gem_group),
           url=map2_chr(srr, run_id, get_valid_url))

df_cells_clean <- df_cells %>%
    dplyr::rename(bx=cell_barcode, target_gene=gene, target_gene_id=gene_id) %>%
    left_join(select(df_runs_clean, gem_group, sample_id)) %>%
    mutate(bx=str_extract(bx, "^[ACGT]{16}"),
           cell_id=str_c(sample_id, "_", bx)) %>%
    select(cell_id, sample_id, gem_group, everything()) %>% select(-c("bx")) %>%
    mutate(across(everything(), as.vector))
```

# Exports
```{r export}
df_runs_clean %>%
    select(sample_id, url) %>%
    dplyr::rename(run_id=sample_id) %>%
    arrange(run_id) %>%
    write_csv(CSV_RUNS_SHEET)

df_runs_clean %>%
    select(sample_id) %>%
    mutate(file_type="bam",
           files=sprintf("input/gwps/bam/%s.bam", sample_id)) %>%
    arrange(sample_id) %>%
    write_csv(CSV_SAMPLE_SHEET)
    
df_cells_clean %>%
    write_csv(CSV_CELL_ANNOTS)
```

---

# Runtime Details
## Session Info
```{r sesh_info, echo=FALSE}
sessionInfo()
```

## Conda Environment
```{bash comment="", echo=FALSE}
if ! command -v conda &> /dev/null
then
  echo "Conda not detected."
elif [ -z "${CONDA_PREFIX}" ]
then
  echo "No active Conda environment."
else
  echo "## Conda Environment YAML"
  conda env export
fi
```
